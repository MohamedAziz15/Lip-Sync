{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedAziz15/Lip-Sync/blob/main/Lip_sync_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-GORAjfAvWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LgWumdr5Rpw",
        "outputId": "e17ea581-03c4-4bd7-fa59-2ddcfb5109e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ffmpeg-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed ffmpeg-python-0.2.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Cloning into 'Wav2Lip'...\n",
            "remote: Enumerating objects: 381, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 381 (delta 0), reused 1 (delta 0), pack-reused 378\u001b[K\n",
            "Receiving objects: 100% (381/381), 534.01 KiB | 1.53 MiB/s, done.\n",
            "Resolving deltas: 100% (210/210), done.\n",
            "/content/Wav2Lip\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1rwFhD1lzrUXJYFjT9xKE7KXbz0CSJ8iI\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "Collecting librosa==0.7.0 (from -r requirements.txt (line 1))\n",
            "  Downloading librosa-0.7.0.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.17.1 (from -r requirements.txt (line 2))\n",
            "  Downloading numpy-1.17.1.zip (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-contrib-python>=4.2.0.34 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.8.0.76)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.1.0.25 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.1.0.25\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Step 1: Set Up the Environment\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install torch torchvision torchaudio opencv-python ffmpeg-python\n",
        "\n",
        "# Clone Wav2Lip repository\n",
        "!git clone https://github.com/Rudrabha/Wav2Lip.git\n",
        "%cd Wav2Lip\n",
        "\n",
        "# Download pre-trained model weights\n",
        "!gdown --id 1rwFhD1lzrUXJYFjT9xKE7KXbz0CSJ8iI -O checkpoints/wav2lip.pth\n",
        "\n",
        "# Install additional dependencies\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ffmpeg-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVC_f24UaxPY",
        "outputId": "0cdae99d-2f87-495a-b164-ada01ebfd9de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqcjOI_0azg2",
        "outputId": "25a51f55-82a3-49cc-b98f-90f1b80c8501"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### handle video as Frames"
      ],
      "metadata": {
        "id": "E-AybJFO8JcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following two methods are taken from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "IMG_SIZE = 224\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ],
      "metadata": {
        "id": "ciEzFS4q8ErH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "\n",
        "    ##take all classlabels from train_df column named 'tag' and store in labels\n",
        "    labels = df[\"tag\"].values\n",
        "\n",
        "    #convert classlabels to label encoding\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
        "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
        "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"train_labels in train set: {train_labels.shape}\")\n",
        "\n",
        "print(f\"test_labels in train set: {test_labels.shape}\")\n",
        "\n",
        "# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters"
      ],
      "metadata": {
        "id": "HO1mRwlKQ4J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WlakJO1IQ4Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Knkq0w-dQ4Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess the Data\n",
        "\n",
        "import cv2\n",
        "import ffmpeg\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from models import Wav2Lip\n",
        "\n",
        "# Function to extract frames from video\n",
        "def extract_frames(video_path):\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    success, image = vidcap.read()\n",
        "    while success:\n",
        "        frames.append(image)\n",
        "        success, image = vidcap.read()\n",
        "    return frames\n",
        "\n",
        "# Function to extract audio from video\n",
        "def extract_audio(video_path):\n",
        "    audio_path = 'audio.wav'\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(video_path)\n",
        "        .output(audio_path)\n",
        "        .run(overwrite_output=True)\n",
        "    )\n",
        "    return audio_path\n",
        "\n",
        "# Function to prepare audio features\n",
        "def get_audio_features(audio_path):\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    return mfcc\n",
        "\n",
        "video_path = '/content/drive/MyDrive/Colab Notebooks/Diverge/Resized_13_K.mp4'\n",
        "frames = extract_frames(video_path)\n",
        "audio_path = extract_audio(video_path)\n",
        "audio_features = get_audio_features(audio_path)"
      ],
      "metadata": {
        "id": "BpyHmzug5fP8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <h1>Step1: Setup Wav2Lip</h1>\n",
        "#@markdown * Install dependency\n",
        "#@markdown * Download pretrained model\n",
        "from IPython.display import HTML, clear_output\n",
        "!rm -rf /content/sample_data\n",
        "!mkdir /content/sample_data\n",
        "\n",
        "!git clone https://github.com/justinjohn0306/Wav2Lip\n",
        "\n",
        "%cd /content/Wav2Lip\n",
        "\n",
        "#download the pretrained model\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/wav2lip.pth' -O 'checkpoints/wav2lip.pth'\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/wav2lip_gan.pth' -O 'checkpoints/wav2lip_gan.pth'\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/resnet50.pth' -O 'checkpoints/resnet50.pth'\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/mobilenet.pth' -O 'checkpoints/mobilenet.pth'\n",
        "a = !pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl\n",
        "!pip install git+https://github.com/elliottzheng/batch-face.git@master\n",
        "\n",
        "!pip install ffmpeg-python mediapipe==0.8.11\n",
        "\n",
        "#this code for recording audio\n",
        "\"\"\"\n",
        "To write this piece of code I took inspiration/code from a lot of places.\n",
        "It was late night, so I'm not sure how much I created or just copied o.O\n",
        "Here are some of the possible references:\n",
        "https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/\n",
        "https://stackoverflow.com/a/18650249\n",
        "https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/\n",
        "https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/\n",
        "https://stackoverflow.com/a/49019356\n",
        "\"\"\"\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "%cd /\n",
        "from ghc.l_ghc_cf import l_ghc_cf\n",
        "%cd content\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "def showVideo(path):\n",
        "  mp4 = open(str(path),'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  return HTML(\"\"\"\n",
        "  <video width=700 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "print(\"All set and ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPtA3NwLb2y3",
        "outputId": "3513c430-3fe7-4818-aaae-cb454e39976f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All set and ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Wav2Lip().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "checkpoint = torch.load('/content/Wav2Lip/checkpoints/wav2lip.pth', map_location='cpu')\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Step 9: Load the Pre-trained Model\n",
        "\n",
        "# model = Wav2Lip().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/Diverge/wav2lip.pth', map_location='cpu')\n",
        "\n",
        "# # Modify the keys in the checkpoint state dictionary to remove the 'module.' prefix\n",
        "# new_state_dict = {}\n",
        "# for k, v in checkpoint['state_dict'].items():\n",
        "#     name = k[7:] # remove 'module.'\n",
        "#     new_state_dict[name] = v\n",
        "\n",
        "# model.load_state_dict(new_state_dict) # Load the modified state dictionary\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "ocl-rlXhseO0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "16b5d955-71e8-4641-db69-d396ec81ff6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Wav2Lip:\n\tMissing key(s) in state_dict: \"face_encoder_blocks.0.0.conv_block.0.weight\", \"face_encoder_blocks.0.0.conv_block.0.bias\", \"face_encoder_blocks.0.0.conv_block.1.weight\", \"face_encoder_blocks.0.0.conv_block.1.bias\", \"face_encoder_blocks.0.0.conv_block.1.running_mean\", \"face_encoder_blocks.0.0.conv_block.1.running_var\", \"face_encoder_blocks.1.0.conv_block.0.weight\", \"face_encoder_blocks.1.0.conv_block.0.bias\", \"face_encoder_blocks.1.0.conv_block.1.weight\", \"face_encoder_blocks.1.0.conv_block.1.bias\", \"face_encoder_blocks.1.0.conv_block.1.running_mean\", \"face_encoder_blocks.1.0.conv_block.1.running_var\", \"face_encoder_blocks.1.1.conv_block.0.weight\", \"face_encoder_blocks.1.1.conv_block.0.bias\", \"face_encoder_blocks.1.1.conv_block.1.weight\", \"face_encoder_blocks.1.1.conv_block.1.bias\", \"face_encoder_blocks.1.1.conv_block.1.running_mean\", \"face_encoder_blocks.1.1.conv_block.1.running_var\", \"face_encoder_blocks.1.2.conv_block.0.weight\", \"face_encoder_blocks.1.2.conv_block.0.bias\", \"face_encoder_blocks.1.2.conv_block.1.weight\", \"face_encoder_blocks.1.2.conv_block.1.bias\", \"face_encoder_blocks.1.2.conv_block.1.running_mean\", \"face_encoder_blocks.1.2.conv_block.1.running_var\", \"face_encoder_blocks.2.0.conv_block.0.weight\", \"face_encoder_blocks.2.0.conv_block.0.bias\", \"face_encoder_blocks.2.0.conv_block.1.weight\", \"face_encoder_blocks.2.0.conv_block.1.bias\", \"face_encoder_blocks.2.0.conv_block.1.running_mean\", \"face_encoder_blocks.2.0.conv_block.1.running_var\", \"face_encoder_blocks.2.1.conv_block.0.weight\", \"face_encoder_blocks.2.1.conv_block.0.bias\", \"face_encoder_blocks.2.1.conv_block.1.weight\", \"face_encoder_blocks.2.1.conv_block.1.bias\", \"face_encoder_blocks.2.1.conv_block.1.running_mean\", \"face_encoder_blocks.2.1.conv_block.1.running_var\", \"face_encoder_blocks.2.2.conv_block.0.weight\", \"face_encoder_blocks.2.2.conv_block.0.bias\", \"face_encoder_blocks.2.2.conv_block.1.weight\", \"face_encoder_blocks.2.2.conv_block.1.bias\", \"face_encoder_blocks.2.2.conv_block.1.running_mean\", \"face_encoder_blocks.2.2.conv_block.1.running_var\", \"face_encoder_blocks.2.3.conv_block.0.weight\", \"face_encoder_blocks.2.3.conv_block.0.bias\", \"face_encoder_blocks.2.3.conv_block.1.weight\", \"face_encoder_blocks.2.3.conv_block.1.bias\", \"face_encoder_blocks.2.3.conv_block.1.running_mean\", \"face_encoder_blocks.2.3.conv_block.1.running_var\", \"face_encoder_blocks.3.0.conv_block.0.weight\", \"face_encoder_blocks.3.0.conv_block.0.bias\", \"face_encoder_blocks.3.0.conv_block.1.weight\", \"face_encoder_blocks.3.0.conv_block.1.bias\", \"face_encoder_blocks.3.0.conv_block.1.running_mean\", \"face_encoder_blocks.3.0.conv_block.1.running_var\", \"face_encoder_blocks.3.1.conv_block.0.weight\", \"face_encoder_blocks.3.1.conv_block.0.bias\", \"face_encoder_blocks.3.1.conv_block.1.weight\", \"face_encoder_blocks.3.1.conv_block.1.bias\", \"face_encoder_blocks.3.1.conv_block.1.running_mean\", \"face_encoder_blocks.3.1.conv_block.1.running_var\", \"face_encoder_blocks.3.2.conv_block.0.weight\", \"face_encoder_blocks.3.2.conv_block.0.bias\", \"face_encoder_blocks.3.2.conv_block.1.weight\", \"face_encoder_blocks.3.2.conv_block.1.bias\", \"face_encoder_blocks.3.2.conv_block.1.running_mean\", \"face_encoder_blocks.3.2.conv_block.1.running_var\", \"face_encoder_blocks.4.0.conv_block.0.weight\", \"face_encoder_blocks.4.0.conv_block.0.bias\", \"face_encoder_blocks.4.0.conv_block.1.weight\", \"face_encoder_blocks.4.0.conv_block.1.bias\", \"face_encoder_blocks.4.0.conv_block.1.running_mean\", \"face_encoder_blocks.4.0.conv_block.1.running_var\", \"face_encoder_blocks.4.1.conv_block.0.weight\", \"face_encoder_blocks.4.1.conv_block.0.bias\", \"face_encoder_blocks.4.1.conv_block.1.weight\", \"face_encoder_blocks.4.1.conv_block.1.bias\", \"face_encoder_blocks.4.1.conv_block.1.running_mean\", \"face_encoder_blocks.4.1.conv_block.1.running_var\", \"face_encoder_blocks.4.2.conv_block.0.weight\", \"face_encoder_blocks.4.2.conv_block.0.bias\", \"face_encoder_blocks.4.2.conv_block.1.weight\", \"face_encoder_blocks.4.2.conv_block.1.bias\", \"face_encoder_blocks.4.2.conv_block.1.running_mean\", \"face_encoder_blocks.4.2.conv_block.1.running_var\", \"face_encoder_blocks.5.0.conv_block.0.weight\", \"face_encoder_blocks.5.0.conv_block.0.bias\", \"face_encoder_blocks.5.0.conv_block.1.weight\", \"face_encoder_blocks.5.0.conv_block.1.bias\", \"face_encoder_blocks.5.0.conv_block.1.running_mean\", \"face_encoder_blocks.5.0.conv_block.1.running_var\", \"face_encoder_blocks.5.1.conv_block.0.weight\", \"face_encoder_blocks.5.1.conv_block.0.bias\", \"face_encoder_blocks.5.1.conv_block.1.weight\", \"face_encoder_blocks.5.1.conv_block.1.bias\", \"face_encoder_blocks.5.1.conv_block.1.running_mean\", \"face_encoder_blocks.5.1.conv_block.1.running_var\", \"face_encoder_blocks.6.0.conv_block.0.weight\", \"face_encoder_blocks.6.0.conv_block.0.bias\", \"face_encoder_blocks.6.0.conv_block.1.weight\", \"face_encoder_blocks.6.0.conv_block.1.bias\", \"face_encoder_blocks.6.0.conv_block.1.running_mean\", \"face_encoder_blocks.6.0.conv_block.1.running_var\", \"face_encoder_blocks.6.1.conv_block.0.weight\", \"face_encoder_blocks.6.1.conv_block.0.bias\", \"face_encoder_blocks.6.1.conv_block.1.weight\", \"face_encoder_blocks.6.1.conv_block.1.bias\", \"face_encoder_blocks.6.1.conv_block.1.running_mean\", \"face_encoder_blocks.6.1.conv_block.1.running_var\", \"audio_encoder.0.conv_block.0.weight\", \"audio_encoder.0.conv_block.0.bias\", \"audio_encoder.0.conv_block.1.weight\", \"audio_encoder.0.conv_block.1.bias\", \"audio_encoder.0.conv_block.1.running_mean\", \"audio_encoder.0.conv_block.1.running_var\", \"audio_encoder.1.conv_block.0.weight\", \"audio_encoder.1.conv_block.0.bias\", \"audio_encoder.1.conv_block.1.weight\", \"audio_encoder.1.conv_block.1.bias\", \"audio_encoder.1.conv_block.1.running_mean\", \"audio_encoder.1.conv_block.1.running_var\", \"audio_encoder.2.conv_block.0.weight\", \"audio_encoder.2.conv_block.0.bias\", \"audio_encoder.2.conv_block.1.weight\", \"audio_encoder.2.conv_block.1.bias\", \"audio_encoder.2.conv_block.1.running_mean\", \"audio_encoder.2.conv_block.1.running_var\", \"audio_encoder.3.conv_block.0.weight\", \"audio_encoder.3.conv_block.0.bias\", \"audio_encoder.3.conv_block.1.weight\", \"audio_encoder.3.conv_block.1.bias\", \"audio_encoder.3.conv_block.1.running_mean\", \"audio_encoder.3.conv_block.1.running_var\", \"audio_encoder.4.conv_block.0.weight\", \"audio_encoder.4.conv_block.0.bias\", \"audio_encoder.4.conv_block.1.weight\", \"audio_encoder.4.conv_block.1.bias\", \"audio_encoder.4.conv_block.1.running_mean\", \"audio_encoder.4.conv_block.1.running_var\", \"audio_encoder.5.conv_block.0.weight\", \"audio_encoder.5.conv_block.0.bias\", \"audio_encoder.5.conv_block.1.weight\", \"audio_encoder.5.conv_block.1.bias\", \"audio_encoder.5.conv_block.1.running_mean\", \"audio_encoder.5.conv_block.1.running_var\", \"audio_encoder.6.conv_block.0.weight\", \"audio_encoder.6.conv_block.0.bias\", \"audio_encoder.6.conv_block.1.weight\", \"audio_encoder.6.conv_block.1.bias\", \"audio_encoder.6.conv_block.1.running_mean\", \"audio_encoder.6.conv_block.1.running_var\", \"audio_encoder.7.conv_block.0.weight\", \"audio_encoder.7.conv_block.0.bias\", \"audio_encoder.7.conv_block.1.weight\", \"audio_encoder.7.conv_block.1.bias\", \"audio_encoder.7.conv_block.1.running_mean\", \"audio_encoder.7.conv_block.1.running_var\", \"audio_encoder.8.conv_block.0.weight\", \"audio_encoder.8.conv_block.0.bias\", \"audio_encoder.8.conv_block.1.weight\", \"audio_encoder.8.conv_block.1.bias\", \"audio_encoder.8.conv_block.1.running_mean\", \"audio_encoder.8.conv_block.1.running_var\", \"audio_encoder.9.conv_block.0.weight\", \"audio_encoder.9.conv_block.0.bias\", \"audio_encoder.9.conv_block.1.weight\", \"audio_encoder.9.conv_block.1.bias\", \"audio_encoder.9.conv_block.1.running_mean\", \"audio_encoder.9.conv_block.1.running_var\", \"audio_encoder.10.conv_block.0.weight\", \"audio_encoder.10.conv_block.0.bias\", \"audio_encoder.10.conv_block.1.weight\", \"audio_encoder.10.conv_block.1.bias\", \"audio_encoder.10.conv_block.1.running_mean\", \"audio_encoder.10.conv_block.1.running_var\", \"audio_encoder.11.conv_block.0.weight\", \"audio_encoder.11.conv_block.0.bias\", \"audio_encoder.11.conv_block.1.weight\", \"audio_encoder.11.conv_block.1.bias\", \"audio_encoder.11.conv_block.1.running_mean\", \"audio_encoder.11.conv_block.1.running_var\", \"audio_encoder.12.conv_block.0.weight\", \"audio_encoder.12.conv_block.0.bias\", \"audio_encoder.12.conv_block.1.weight\", \"audio_encoder.12.conv_block.1.bias\", \"audio_encoder.12.conv_block.1.running_mean\", \"audio_encoder.12.conv_block.1.running_var\", \"face_decoder_blocks.0.0.conv_block.0.weight\", \"face_decoder_blocks.0.0.conv_block.0.bias\", \"face_decoder_blocks.0.0.conv_block.1.weight\", \"face_decoder_blocks.0.0.conv_block.1.bias\", \"face_decoder_blocks.0.0.conv_block.1.running_mean\", \"face_decoder_blocks.0.0.conv_block.1.running_var\", \"face_decoder_blocks.1.0.conv_block.0.weight\", \"face_decoder_blocks.1.0.conv_block.0.bias\", \"face_decoder_blocks.1.0.conv_block.1.weight\", \"face_decoder_blocks.1.0.conv_block.1.bias\", \"face_decoder_blocks.1.0.conv_block.1.running_mean\", \"face_decoder_blocks.1.0.conv_block.1.running_var\", \"face_decoder_blocks.1.1.conv_block.0.weight\", \"face_decoder_blocks.1.1.conv_block.0.bias\", \"face_decoder_blocks.1.1.conv_block.1.weight\", \"face_decoder_blocks.1.1.conv_block.1.bias\", \"face_decoder_blocks.1.1.conv_block.1.running_mean\", \"face_decoder_blocks.1.1.conv_block.1.running_var\", \"face_decoder_blocks.2.0.conv_block.0.weight\", \"face_decoder_blocks.2.0.conv_block.0.bias\", \"face_decoder_blocks.2.0.conv_block.1.weight\", \"face_decoder_blocks.2.0.conv_block.1.bias\", \"face_decoder_blocks.2.0.conv_block.1.running_mean\", \"face_decoder_blocks.2.0.conv_block.1.running_var\", \"face_decoder_blocks.2.1.conv_block.0.weight\", \"face_decoder_blocks.2.1.conv_block.0.bias\", \"face_decoder_blocks.2.1.conv_block.1.weight\", \"face_decoder_blocks.2.1.conv_block.1.bias\", \"face_decoder_blocks.2.1.conv_block.1.running_mean\", \"face_decoder_blocks.2.1.conv_block.1.running_var\", \"face_decoder_blocks.2.2.conv_block.0.weight\", \"face_decoder_blocks.2.2.conv_block.0.bias\", \"face_decoder_blocks.2.2.conv_block.1.weight\", \"face_decoder_blocks.2.2.conv_block.1.bias\", \"face_decoder_blocks.2.2.conv_block.1.running_mean\", \"face_decoder_blocks.2.2.conv_block.1.running_var\", \"face_decoder_blocks.3.0.conv_block.0.weight\", \"face_decoder_blocks.3.0.conv_block.0.bias\", \"face_decoder_blocks.3.0.conv_block.1.weight\", \"face_decoder_blocks.3.0.conv_block.1.bias\", \"face_decoder_blocks.3.0.conv_block.1.running_mean\", \"face_decoder_blocks.3.0.conv_block.1.running_var\", \"face_decoder_blocks.3.1.conv_block.0.weight\", \"face_decoder_blocks.3.1.conv_block.0.bias\", \"face_decoder_blocks.3.1.conv_block.1.weight\", \"face_decoder_blocks.3.1.conv_block.1.bias\", \"face_decoder_blocks.3.1.conv_block.1.running_mean\", \"face_decoder_blocks.3.1.conv_block.1.running_var\", \"face_decoder_blocks.3.2.conv_block.0.weight\", \"face_decoder_blocks.3.2.conv_block.0.bias\", \"face_decoder_blocks.3.2.conv_block.1.weight\", \"face_decoder_blocks.3.2.conv_block.1.bias\", \"face_decoder_blocks.3.2.conv_block.1.running_mean\", \"face_decoder_blocks.3.2.conv_block.1.running_var\", \"face_decoder_blocks.4.0.conv_block.0.weight\", \"face_decoder_blocks.4.0.conv_block.0.bias\", \"face_decoder_blocks.4.0.conv_block.1.weight\", \"face_decoder_blocks.4.0.conv_block.1.bias\", \"face_decoder_blocks.4.0.conv_block.1.running_mean\", \"face_decoder_blocks.4.0.conv_block.1.running_var\", \"face_decoder_blocks.4.1.conv_block.0.weight\", \"face_decoder_blocks.4.1.conv_block.0.bias\", \"face_decoder_blocks.4.1.conv_block.1.weight\", \"face_decoder_blocks.4.1.conv_block.1.bias\", \"face_decoder_blocks.4.1.conv_block.1.running_mean\", \"face_decoder_blocks.4.1.conv_block.1.running_var\", \"face_decoder_blocks.4.2.conv_block.0.weight\", \"face_decoder_blocks.4.2.conv_block.0.bias\", \"face_decoder_blocks.4.2.conv_block.1.weight\", \"face_decoder_blocks.4.2.conv_block.1.bias\", \"face_decoder_blocks.4.2.conv_block.1.running_mean\", \"face_decoder_blocks.4.2.conv_block.1.running_var\", \"face_decoder_blocks.5.0.conv_block.0.weight\", \"face_decoder_blocks.5.0.conv_block.0.bias\", \"face_decoder_blocks.5.0.conv_block.1.weight\", \"face_decoder_blocks.5.0.conv_block.1.bias\", \"face_decoder_blocks.5.0.conv_block.1.running_mean\", \"face_decoder_blocks.5.0.conv_block.1.running_var\", \"face_decoder_blocks.5.1.conv_block.0.weight\", \"face_decoder_blocks.5.1.conv_block.0.bias\", \"face_decoder_blocks.5.1.conv_block.1.weight\", \"face_decoder_blocks.5.1.conv_block.1.bias\", \"face_decoder_blocks.5.1.conv_block.1.running_mean\", \"face_decoder_blocks.5.1.conv_block.1.running_var\", \"face_decoder_blocks.5.2.conv_block.0.weight\", \"face_decoder_blocks.5.2.conv_block.0.bias\", \"face_decoder_blocks.5.2.conv_block.1.weight\", \"face_decoder_blocks.5.2.conv_block.1.bias\", \"face_decoder_blocks.5.2.conv_block.1.running_mean\", \"face_decoder_blocks.5.2.conv_block.1.running_var\", \"face_decoder_blocks.6.0.conv_block.0.weight\", \"face_decoder_blocks.6.0.conv_block.0.bias\", \"face_decoder_blocks.6.0.conv_block.1.weight\", \"face_decoder_blocks.6.0.conv_block.1.bias\", \"face_decoder_blocks.6.0.conv_block.1.running_mean\", \"face_decoder_blocks.6.0.conv_block.1.running_var\", \"face_decoder_blocks.6.1.conv_block.0.weight\", \"face_decoder_blocks.6.1.conv_block.0.bias\", \"face_decoder_blocks.6.1.conv_block.1.weight\", \"face_decoder_blocks.6.1.conv_block.1.bias\", \"face_decoder_blocks.6.1.conv_block.1.running_mean\", \"face_decoder_blocks.6.1.conv_block.1.running_var\", \"face_decoder_blocks.6.2.conv_block.0.weight\", \"face_decoder_blocks.6.2.conv_block.0.bias\", \"face_decoder_blocks.6.2.conv_block.1.weight\", \"face_decoder_blocks.6.2.conv_block.1.bias\", \"face_decoder_blocks.6.2.conv_block.1.running_mean\", \"face_decoder_blocks.6.2.conv_block.1.running_var\", \"output_block.0.conv_block.0.weight\", \"output_block.0.conv_block.0.bias\", \"output_block.0.conv_block.1.weight\", \"output_block.0.conv_block.1.bias\", \"output_block.0.conv_block.1.running_mean\", \"output_block.0.conv_block.1.running_var\", \"output_block.1.weight\", \"output_block.1.bias\". \n\tUnexpected key(s) in state_dict: \"module.face_encoder_blocks.0.0.conv_block.0.weight\", \"module.face_encoder_blocks.0.0.conv_block.0.bias\", \"module.face_encoder_blocks.0.0.conv_block.1.weight\", \"module.face_encoder_blocks.0.0.conv_block.1.bias\", \"module.face_encoder_blocks.0.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.0.0.conv_block.1.running_var\", \"module.face_encoder_blocks.0.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.0.conv_block.0.weight\", \"module.face_encoder_blocks.1.0.conv_block.0.bias\", \"module.face_encoder_blocks.1.0.conv_block.1.weight\", \"module.face_encoder_blocks.1.0.conv_block.1.bias\", \"module.face_encoder_blocks.1.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.0.conv_block.1.running_var\", \"module.face_encoder_blocks.1.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.1.conv_block.0.weight\", \"module.face_encoder_blocks.1.1.conv_block.0.bias\", \"module.face_encoder_blocks.1.1.conv_block.1.weight\", \"module.face_encoder_blocks.1.1.conv_block.1.bias\", \"module.face_encoder_blocks.1.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.1.conv_block.1.running_var\", \"module.face_encoder_blocks.1.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.2.conv_block.0.weight\", \"module.face_encoder_blocks.1.2.conv_block.0.bias\", \"module.face_encoder_blocks.1.2.conv_block.1.weight\", \"module.face_encoder_blocks.1.2.conv_block.1.bias\", \"module.face_encoder_blocks.1.2.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.2.conv_block.1.running_var\", \"module.face_encoder_blocks.1.2.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.2.0.conv_block.0.weight\", \"module.face_encoder_blocks.2.0.conv_block.0.bias\", \"module.face_encoder_blocks.2.0.conv_block.1.weight\", \"module.face_encoder_blocks.2.0.conv_block.1.bias\", \"module.face_encoder_blocks.2.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.2.0.conv_block.1.running_var\", \"module.face_encoder_blocks.2.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.2.1.conv_block.0.weight\", \"module.face_encoder_blocks.2.1.conv_block.0.bias\", \"module.face_encoder_blocks.2.1.conv_block.1.weight\", \"module.face_encoder_blocks.2.1.conv_block.1.bias\", \"module.face_encoder_blocks.2.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.2.1.conv_block.1.running_var\", \"module.face_encoder_blocks.2.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.2.2.conv_block.0.weight\", \"module.face_encoder_blocks.2.2.conv_block.0.bias\", \"module.face_encoder_blocks.2.2.conv_block.1.weight\", \"module.face_encoder_blocks.2.2.conv_block.1.bias\", \"module.face_encoder_blocks.2.2.conv_block.1.running_mean\", \"module.face_encoder_blocks.2.2.conv_block.1.running_var\", \"module.face_encoder_blocks.2.2.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.2.3.conv_block.0.weight\", \"module.face_encoder_blocks.2.3.conv_block.0.bias\", \"module.face_encoder_blocks.2.3.conv_block.1.weight\", \"module.face_encoder_blocks.2.3.conv_block.1.bias\", \"module.face_encoder_blocks.2.3.conv_block.1.running_mean\", \"module.face_encoder_blocks.2.3.conv_block.1.running_var\", \"module.face_encoder_blocks.2.3.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.3.0.conv_block.0.weight\", \"module.face_encoder_blocks.3.0.conv_block.0.bias\", \"module.face_encoder_blocks.3.0.conv_block.1.weight\", \"module.face_encoder_blocks.3.0.conv_block.1.bias\", \"module.face_encoder_blocks.3.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.3.0.conv_block.1.running_var\", \"module.face_encoder_blocks.3.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.3.1.conv_block.0.weight\", \"module.face_encoder_blocks.3.1.conv_block.0.bias\", \"module.face_encoder_blocks.3.1.conv_block.1.weight\", \"module.face_encoder_blocks.3.1.conv_block.1.bias\", \"module.face_encoder_blocks.3.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.3.1.conv_block.1.running_var\", \"module.face_encoder_blocks.3.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.3.2.conv_block.0.weight\", \"module.face_encoder_blocks.3.2.conv_block.0.bias\", \"module.face_encoder_blocks.3.2.conv_block.1.weight\", \"module.face_encoder_blocks.3.2.conv_block.1.bias\", \"module.face_encoder_blocks.3.2.conv_block.1.running_mean\", \"module.face_encoder_blocks.3.2.conv_block.1.running_var\", \"module.face_encoder_blocks.3.2.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.4.0.conv_block.0.weight\", \"module.face_encoder_blocks.4.0.conv_block.0.bias\", \"module.face_encoder_blocks.4.0.conv_block.1.weight\", \"module.face_encoder_blocks.4.0.conv_block.1.bias\", \"module.face_encoder_blocks.4.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.4.0.conv_block.1.running_var\", \"module.face_encoder_blocks.4.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.4.1.conv_block.0.weight\", \"module.face_encoder_blocks.4.1.conv_block.0.bias\", \"module.face_encoder_blocks.4.1.conv_block.1.weight\", \"module.face_encoder_blocks.4.1.conv_block.1.bias\", \"module.face_encoder_blocks.4.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.4.1.conv_block.1.running_var\", \"module.face_encoder_blocks.4.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.4.2.conv_block.0.weight\", \"module.face_encoder_blocks.4.2.conv_block.0.bias\", \"module.face_encoder_blocks.4.2.conv_block.1.weight\", \"module.face_encoder_blocks.4.2.conv_block.1.bias\", \"module.face_encoder_blocks.4.2.conv_block.1.running_mean\", \"module.face_encoder_blocks.4.2.conv_block.1.running_var\", \"module.face_encoder_blocks.4.2.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.5.0.conv_block.0.weight\", \"module.face_encoder_blocks.5.0.conv_block.0.bias\", \"module.face_encoder_blocks.5.0.conv_block.1.weight\", \"module.face_encoder_blocks.5.0.conv_block.1.bias\", \"module.face_encoder_blocks.5.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.5.0.conv_block.1.running_var\", \"module.face_encoder_blocks.5.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.5.1.conv_block.0.weight\", \"module.face_encoder_blocks.5.1.conv_block.0.bias\", \"module.face_encoder_blocks.5.1.conv_block.1.weight\", \"module.face_encoder_blocks.5.1.conv_block.1.bias\", \"module.face_encoder_blocks.5.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.5.1.conv_block.1.running_var\", \"module.face_encoder_blocks.5.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.6.0.conv_block.0.weight\", \"module.face_encoder_blocks.6.0.conv_block.0.bias\", \"module.face_encoder_blocks.6.0.conv_block.1.weight\", \"module.face_encoder_blocks.6.0.conv_block.1.bias\", \"module.face_encoder_blocks.6.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.6.0.conv_block.1.running_var\", \"module.face_encoder_blocks.6.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.6.1.conv_block.0.weight\", \"module.face_encoder_blocks.6.1.conv_block.0.bias\", \"module.face_encoder_blocks.6.1.conv_block.1.weight\", \"module.face_encoder_blocks.6.1.conv_block.1.bias\", \"module.face_encoder_blocks.6.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.6.1.conv_block.1.running_var\", \"module.face_encoder_blocks.6.1.conv_block.1.num_batches_tracked\", \"module.audio_encoder.0.conv_block.0.weight\", \"module.audio_encoder.0.conv_block.0.bias\", \"module.audio_encoder.0.conv_block.1.weight\", \"module.audio_encoder.0.conv_block.1.bias\", \"module.audio_encoder.0.conv_block.1.running_mean\", \"module.audio_encoder.0.conv_block.1.running_var\", \"module.audio_encoder.0.conv_block.1.num_batches_tracked\", \"module.audio_encoder.1.conv_block.0.weight\", \"module.audio_encoder.1.conv_block.0.bias\", \"module.audio_encoder.1.conv_block.1.weight\", \"module.audio_encoder.1.conv_block.1.bias\", \"module.audio_encoder.1.conv_block.1.running_mean\", \"module.audio_encoder.1.conv_block.1.running_var\", \"module.audio_encoder.1.conv_block.1.num_batches_tracked\", \"module.audio_encoder.2.conv_block.0.weight\", \"module.audio_encoder.2.conv_block.0.bias\", \"module.audio_encoder.2.conv_block.1.weight\", \"module.audio_encoder.2.conv_block.1.bias\", \"module.audio_encoder.2.conv_block.1.running_mean\", \"module.audio_encoder.2.conv_block.1.running_var\", \"module.audio_encoder.2.conv_block.1.num_batches_tracked\", \"module.audio_encoder.3.conv_block.0.weight\", \"module.audio_encoder.3.conv_block.0.bias\", \"module.audio_encoder.3.conv_block.1.weight\", \"module.audio_encoder.3.conv_block.1.bias\", \"module.audio_encoder.3.conv_block.1.running_mean\", \"module.audio_encoder.3.conv_block.1.running_var\", \"module.audio_encoder.3.conv_block.1.num_batches_tracked\", \"module.audio_encoder.4.conv_block.0.weight\", \"module.audio_encoder.4.conv_block.0.bias\", \"module.audio_encoder.4.conv_block.1.weight\", \"module.audio_encoder.4.conv_block.1.bias\", \"module.audio_encoder.4.conv_block.1.running_mean\", \"module.audio_encoder.4.conv_block.1.running_var\", \"module.audio_encoder.4.conv_block.1.num_batches_tracked\", \"module.audio_encoder.5.conv_block.0.weight\", \"module.audio_encoder.5.conv_block.0.bias\", \"module.audio_encoder.5.conv_block.1.weight\", \"module.audio_encoder.5.conv_block.1.bias\", \"module.audio_encoder.5.conv_block.1.running_mean\", \"module.audio_encoder.5.conv_block.1.running_var\", \"module.audio_encoder.5.conv_block.1.num_batches_tracked\", \"module.audio_encoder.6.conv_block.0.weight\", \"module.audio_encoder.6.conv_block.0.bias\", \"module.audio_encoder.6.conv_block.1.weight\", \"module.audio_encoder.6.conv_block.1.bias\", \"module.audio_encoder.6.conv_block.1.running_mean\", \"module.audio_encoder.6.conv_block.1.running_var\", \"module.audio_encoder.6.conv_block.1.num_batches_tracked\", \"module.audio_encoder.7.conv_block.0.weight\", \"module.audio_encoder.7.conv_block.0.bias\", \"module.audio_encoder.7.conv_block.1.weight\", \"module.audio_encoder.7.conv_block.1.bias\", \"module.audio_encoder.7.conv_block.1.running_mean\", \"module.audio_encoder.7.conv_block.1.running_var\", \"module.audio_encoder.7.conv_block.1.num_batches_tracked\", \"module.audio_encoder.8.conv_block.0.weight\", \"module.audio_encoder.8.conv_block.0.bias\", \"module.audio_encoder.8.conv_block.1.weight\", \"module.audio_encoder.8.conv_block.1.bias\", \"module.audio_encoder.8.conv_block.1.running_mean\", \"module.audio_encoder.8.conv_block.1.running_var\", \"module.audio_encoder.8.conv_block.1.num_batches_tracked\", \"module.audio_encoder.9.conv_block.0.weight\", \"module.audio_encoder.9.conv_block.0.bias\", \"module.audio_encoder.9.conv_block.1.weight\", \"module.audio_encoder.9.conv_block.1.bias\", \"module.audio_encoder.9.conv_block.1.running_mean\", \"module.audio_encoder.9.conv_block.1.running_var\", \"module.audio_encoder.9.conv_block.1.num_batches_tracked\", \"module.audio_encoder.10.conv_block.0.weight\", \"module.audio_encoder.10.conv_block.0.bias\", \"module.audio_encoder.10.conv_block.1.weight\", \"module.audio_encoder.10.conv_block.1.bias\", \"module.audio_encoder.10.conv_block.1.running_mean\", \"module.audio_encoder.10.conv_block.1.running_var\", \"module.audio_encoder.10.conv_block.1.num_batches_tracked\", \"module.audio_encoder.11.conv_block.0.weight\", \"module.audio_encoder.11.conv_block.0.bias\", \"module.audio_encoder.11.conv_block.1.weight\", \"module.audio_encoder.11.conv_block.1.bias\", \"module.audio_encoder.11.conv_block.1.running_mean\", \"module.audio_encoder.11.conv_block.1.running_var\", \"module.audio_encoder.11.conv_block.1.num_batches_tracked\", \"module.audio_encoder.12.conv_block.0.weight\", \"module.audio_encoder.12.conv_block.0.bias\", \"module.audio_encoder.12.conv_block.1.weight\", \"module.audio_encoder.12.conv_block.1.bias\", \"module.audio_encoder.12.conv_block.1.running_mean\", \"module.audio_encoder.12.conv_block.1.running_var\", \"module.audio_encoder.12.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.0.0.conv_block.0.weight\", \"module.face_decoder_blocks.0.0.conv_block.0.bias\", \"module.face_decoder_blocks.0.0.conv_block.1.weight\", \"module.face_decoder_blocks.0.0.conv_block.1.bias\", \"module.face_decoder_blocks.0.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.0.0.conv_block.1.running_var\", \"module.face_decoder_blocks.0.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.1.0.conv_block.0.weight\", \"module.face_decoder_blocks.1.0.conv_block.0.bias\", \"module.face_decoder_blocks.1.0.conv_block.1.weight\", \"module.face_decoder_blocks.1.0.conv_block.1.bias\", \"module.face_decoder_blocks.1.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.1.0.conv_block.1.running_var\", \"module.face_decoder_blocks.1.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.1.1.conv_block.0.weight\", \"module.face_decoder_blocks.1.1.conv_block.0.bias\", \"module.face_decoder_blocks.1.1.conv_block.1.weight\", \"module.face_decoder_blocks.1.1.conv_block.1.bias\", \"module.face_decoder_blocks.1.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.1.1.conv_block.1.running_var\", \"module.face_decoder_blocks.1.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.2.0.conv_block.0.weight\", \"module.face_decoder_blocks.2.0.conv_block.0.bias\", \"module.face_decoder_blocks.2.0.conv_block.1.weight\", \"module.face_decoder_blocks.2.0.conv_block.1.bias\", \"module.face_decoder_blocks.2.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.2.0.conv_block.1.running_var\", \"module.face_decoder_blocks.2.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.2.1.conv_block.0.weight\", \"module.face_decoder_blocks.2.1.conv_block.0.bias\", \"module.face_decoder_blocks.2.1.conv_block.1.weight\", \"module.face_decoder_blocks.2.1.conv_block.1.bias\", \"module.face_decoder_blocks.2.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.2.1.conv_block.1.running_var\", \"module.face_decoder_blocks.2.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.2.2.conv_block.0.weight\", \"module.face_decoder_blocks.2.2.conv_block.0.bias\", \"module.face_decoder_blocks.2.2.conv_block.1.weight\", \"module.face_decoder_blocks.2.2.conv_block.1.bias\", \"module.face_decoder_blocks.2.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.2.2.conv_block.1.running_var\", \"module.face_decoder_blocks.2.2.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.3.0.conv_block.0.weight\", \"module.face_decoder_blocks.3.0.conv_block.0.bias\", \"module.face_decoder_blocks.3.0.conv_block.1.weight\", \"module.face_decoder_blocks.3.0.conv_block.1.bias\", \"module.face_decoder_blocks.3.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.3.0.conv_block.1.running_var\", \"module.face_decoder_blocks.3.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.3.1.conv_block.0.weight\", \"module.face_decoder_blocks.3.1.conv_block.0.bias\", \"module.face_decoder_blocks.3.1.conv_block.1.weight\", \"module.face_decoder_blocks.3.1.conv_block.1.bias\", \"module.face_decoder_blocks.3.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.3.1.conv_block.1.running_var\", \"module.face_decoder_blocks.3.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.3.2.conv_block.0.weight\", \"module.face_decoder_blocks.3.2.conv_block.0.bias\", \"module.face_decoder_blocks.3.2.conv_block.1.weight\", \"module.face_decoder_blocks.3.2.conv_block.1.bias\", \"module.face_decoder_blocks.3.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.3.2.conv_block.1.running_var\", \"module.face_decoder_blocks.3.2.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.4.0.conv_block.0.weight\", \"module.face_decoder_blocks.4.0.conv_block.0.bias\", \"module.face_decoder_blocks.4.0.conv_block.1.weight\", \"module.face_decoder_blocks.4.0.conv_block.1.bias\", \"module.face_decoder_blocks.4.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.4.0.conv_block.1.running_var\", \"module.face_decoder_blocks.4.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.4.1.conv_block.0.weight\", \"module.face_decoder_blocks.4.1.conv_block.0.bias\", \"module.face_decoder_blocks.4.1.conv_block.1.weight\", \"module.face_decoder_blocks.4.1.conv_block.1.bias\", \"module.face_decoder_blocks.4.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.4.1.conv_block.1.running_var\", \"module.face_decoder_blocks.4.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.4.2.conv_block.0.weight\", \"module.face_decoder_blocks.4.2.conv_block.0.bias\", \"module.face_decoder_blocks.4.2.conv_block.1.weight\", \"module.face_decoder_blocks.4.2.conv_block.1.bias\", \"module.face_decoder_blocks.4.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.4.2.conv_block.1.running_var\", \"module.face_decoder_blocks.4.2.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.5.0.conv_block.0.weight\", \"module.face_decoder_blocks.5.0.conv_block.0.bias\", \"module.face_decoder_blocks.5.0.conv_block.1.weight\", \"module.face_decoder_blocks.5.0.conv_block.1.bias\", \"module.face_decoder_blocks.5.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.5.0.conv_block.1.running_var\", \"module.face_decoder_blocks.5.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.5.1.conv_block.0.weight\", \"module.face_decoder_blocks.5.1.conv_block.0.bias\", \"module.face_decoder_blocks.5.1.conv_block.1.weight\", \"module.face_decoder_blocks.5.1.conv_block.1.bias\", \"module.face_decoder_blocks.5.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.5.1.conv_block.1.running_var\", \"module.face_decoder_blocks.5.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.5.2.conv_block.0.weight\", \"module.face_decoder_blocks.5.2.conv_block.0.bias\", \"module.face_decoder_blocks.5.2.conv_block.1.weight\", \"module.face_decoder_blocks.5.2.conv_block.1.bias\", \"module.face_decoder_blocks.5.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.5.2.conv_block.1.running_var\", \"module.face_decoder_blocks.5.2.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.6.0.conv_block.0.weight\", \"module.face_decoder_blocks.6.0.conv_block.0.bias\", \"module.face_decoder_blocks.6.0.conv_block.1.weight\", \"module.face_decoder_blocks.6.0.conv_block.1.bias\", \"module.face_decoder_blocks.6.0.conv_block.1.running_mean\", \"module.face_decoder_blocks.6.0.conv_block.1.running_var\", \"module.face_decoder_blocks.6.0.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.6.1.conv_block.0.weight\", \"module.face_decoder_blocks.6.1.conv_block.0.bias\", \"module.face_decoder_blocks.6.1.conv_block.1.weight\", \"module.face_decoder_blocks.6.1.conv_block.1.bias\", \"module.face_decoder_blocks.6.1.conv_block.1.running_mean\", \"module.face_decoder_blocks.6.1.conv_block.1.running_var\", \"module.face_decoder_blocks.6.1.conv_block.1.num_batches_tracked\", \"module.face_decoder_blocks.6.2.conv_block.0.weight\", \"module.face_decoder_blocks.6.2.conv_block.0.bias\", \"module.face_decoder_blocks.6.2.conv_block.1.weight\", \"module.face_decoder_blocks.6.2.conv_block.1.bias\", \"module.face_decoder_blocks.6.2.conv_block.1.running_mean\", \"module.face_decoder_blocks.6.2.conv_block.1.running_var\", \"module.face_decoder_blocks.6.2.conv_block.1.num_batches_tracked\", \"module.output_block.0.conv_block.0.weight\", \"module.output_block.0.conv_block.0.bias\", \"module.output_block.0.conv_block.1.weight\", \"module.output_block.0.conv_block.1.bias\", \"module.output_block.0.conv_block.1.running_mean\", \"module.output_block.0.conv_block.1.running_var\", \"module.output_block.0.conv_block.1.num_batches_tracked\", \"module.output_block.1.weight\", \"module.output_block.1.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9b113b7e462c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWav2Lip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Wav2Lip/checkpoints/wav2lip.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2190\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Wav2Lip:\n\tMissing key(s) in state_dict: \"face_encoder_blocks.0.0.conv_block.0.weight\", \"face_encoder_blocks.0.0.conv_block.0.bias\", \"face_encoder_blocks.0.0.conv_block.1.weight\", \"face_encoder_blocks.0.0.conv_block.1.bias\", \"face_encoder_blocks.0.0.conv_block.1.running_mean\", \"face_encoder_blocks.0.0.conv_block.1.running_var\", \"face_encoder_blocks.1.0.conv_block.0.weight\", \"face_encoder_blocks.1.0.conv_block.0.bias\", \"face_encoder_blocks.1.0.conv_block.1.weight\", \"face_encoder_blocks.1.0.conv_block.1.bias\", \"face_encoder_blocks.1.0.conv_block.1.running_mean\", \"face_encoder_blocks.1.0.conv_block.1.running_var\", \"face_encoder_blocks.1.1.conv_block.0.weight\", \"face_encoder_blocks.1.1.conv_block.0.bias\", \"face_encoder_blocks.1.1.conv_block.1.weight\", \"face_encoder_blocks.1.1.conv_block.1.bias\", \"face_encoder_blocks.1.1.conv_block.1.running_mean\", \"face_encoder_blocks.1.1.conv_block.1.running_var\", \"face_encoder_blocks.1.2.conv_block.0.weight\", \"face_encoder_blocks.1.2.conv_block.0.bias\", \"face_encoder_blocks.1.2.conv_block.1.weight\", \"face_encoder_blocks.1.2.conv_block.1.bias\", \"face_encoder_blocks.1.2.conv_block.1.running_mean\", \"face_encoder_blocks.1.2.conv_block.1.running_var\", \"face_encoder_blocks.2.0.conv_block.0.weight\", \"face_encoder_blocks.2.0.conv_block.0.bias\", \"face_encoder_blocks.2.0.conv_block.1.weight\", \"face_encoder_blocks.2.0.conv_block.1.bias\", \"face_encoder_blocks.2.0.conv_block.1.running_mean\", \"face_encoder_blocks.2.0.conv_block.1.running_var\", \"face_encoder_blocks....\n\tUnexpected key(s) in state_dict: \"module.face_encoder_blocks.0.0.conv_block.0.weight\", \"module.face_encoder_blocks.0.0.conv_block.0.bias\", \"module.face_encoder_blocks.0.0.conv_block.1.weight\", \"module.face_encoder_blocks.0.0.conv_block.1.bias\", \"module.face_encoder_blocks.0.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.0.0.conv_block.1.running_var\", \"module.face_encoder_blocks.0.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.0.conv_block.0.weight\", \"module.face_encoder_blocks.1.0.conv_block.0.bias\", \"module.face_encoder_blocks.1.0.conv_block.1.weight\", \"module.face_encoder_blocks.1.0.conv_block.1.bias\", \"module.face_encoder_blocks.1.0.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.0.conv_block.1.running_var\", \"module.face_encoder_blocks.1.0.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.1.conv_block.0.weight\", \"module.face_encoder_blocks.1.1.conv_block.0.bias\", \"module.face_encoder_blocks.1.1.conv_block.1.weight\", \"module.face_encoder_blocks.1.1.conv_block.1.bias\", \"module.face_encoder_blocks.1.1.conv_block.1.running_mean\", \"module.face_encoder_blocks.1.1.conv_block.1.running_var\", \"module.face_encoder_blocks.1.1.conv_block.1.num_batches_tracked\", \"module.face_encoder_blocks.1.2.conv_block.0.weight\", \"module.face_encoder_blocks.1.2.conv_block.0.bias\", \"module.face_encoder_blocks.1.2.conv_block.1.weight\", \"module.face_encoder_blocks.1.2.conv_block.1.bias\", \"module.face_encoder_blocks.1.2.conv_block.1.running_mean..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load and Run the Model\n",
        "\n",
        "# Function to generate lip-synced frames\n",
        "def generate_lip_synced_frames(model, frames, audio_features):\n",
        "    synced_frames = []\n",
        "    for i in range(len(frames)):\n",
        "        frame = frames[i]\n",
        "        audio_feature = audio_features[:, i:i+80]  # Assuming 80 frames per second\n",
        "        audio_feature = torch.FloatTensor(audio_feature).unsqueeze(0).to('cuda')\n",
        "        #frame = torch.FloatTensor(frame).unsqueeze(0).to('cuda')\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Convert to grayscale\n",
        "        frame = torch.FloatTensor(frame).unsqueeze(0).to('cuda')\n",
        "        # frame = torch.FloatTensor(frame).unsqueeze(0).permute(0, 3, 1, 2).to('cuda')\n",
        "\n",
        "        synced_frame = model(frame, audio_feature)\n",
        "        synced_frames.append(synced_frame.cpu().numpy())\n",
        "    return synced_frames\n",
        "synced_frames = generate_lip_synced_frames(model, frames, audio_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "rkHUdcu5CIIv",
        "outputId": "6decb6ed-5e7c-4b89-e1ab-e37a85a08529"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "expected 4D input (got 3D input)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6f5a375b9b58>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msynced_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynced_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msynced_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msynced_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_lip_synced_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-6f5a375b9b58>\u001b[0m in \u001b[0;36mgenerate_lip_synced_frames\u001b[0;34m(model, frames, audio_features)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# frame = torch.FloatTensor(frame).unsqueeze(0).permute(0, 3, 1, 2).to('cuda')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msynced_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0msynced_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynced_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msynced_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Wav2Lip/models/wav2lip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, audio_sequences, face_sequences)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mface_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mface_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0maudio_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_sequences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B, 512, 1, 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Wav2Lip/models/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# exponential_average_factor is set to self.momentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m_check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"expected 4D input (got {input.dim()}D input)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Combine Frames and Audio to Generate the Final Video\n",
        "\n",
        "# Function to save video from frames\n",
        "def save_video(frames, output_path, fps=25):\n",
        "    height, width, layers = frames[0].shape\n",
        "    video = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))\n",
        "    for frame in frames:\n",
        "        video.write(frame)\n",
        "    video.release()\n",
        "\n",
        "output_video_path = 'output_video.mp4'\n",
        "save_video(synced_frames, output_video_path)\n",
        "\n",
        "# Add audio to the video\n",
        "output_video_with_audio = 'output_video_with_audio.mp4'\n",
        "(\n",
        "    ffmpeg\n",
        "    .input(output_video_path)\n",
        "    .input(audio_path)\n",
        "    .output(output_video_with_audio)\n",
        "    .run(overwrite_output=True)\n",
        ")\n",
        "\n",
        "# Step 5: Evaluate and Fine-Tune\n",
        "\n",
        "print(\"Lip-syncing completed and saved to\", output_video_with_audio)\n"
      ],
      "metadata": {
        "id": "HcDL8Taf5fL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FK9RrHHu5fJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mWDVffs5fGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RbJfY3oz5gZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load and Run the Model\n",
        "\n",
        "# model = Wav2Lip().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/Diverge/wav2lip.pth', map_location='cpu')\n",
        "# model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# model.load_state_dict({k.replace(\"module.\", \"\"): v for k, v in checkpoint.items()})\n",
        "\n",
        "\n",
        "# model_save_location = \"/content/drive/MyDrive/Colab Notebooks/Diverge/wav2lip.pth\"\n",
        "\n",
        "# state_dict = torch.load(model_save_location, map_location='cpu')\n",
        "# model.load_state_dict({k.replace(\"module.\", \"\"): v for k, v in state_dict.items()})\n",
        "\n",
        "\n",
        "# # Function to generate lip-synced frames\n",
        "# def generate_lip_synced_frames(model, frames, audio_features):\n",
        "#     synced_frames = []\n",
        "#     for i in range(len(frames)):\n",
        "#         frame = frames[i]\n",
        "#         audio_feature = audio_features[:, i:i+80]  # Assuming 80 frames per second\n",
        "#         audio_feature = torch.FloatTensor(audio_feature).unsqueeze(0).to('cuda')\n",
        "#         frame = torch.FloatTensor(frame).unsqueeze(0).to('cuda')\n",
        "#         synced_frame = model(frame, audio_feature)\n",
        "#         synced_frames.append(synced_frame.cpu().numpy())\n",
        "#     return synced_frames\n",
        "# synced_frames = generate_lip_synced_frames(model, frames, audio_features)\n",
        "##########################################################################################\n",
        "# Step 3: Load and Run the Model\n",
        "\n",
        "# ... (previous code)\n",
        "\n",
        "# # Function to generate lip-synced frames\n",
        "# def generate_lip_synced_frames(model, frames, audio_features):\n",
        "#     synced_frames = []\n",
        "#     for i in range(len(frames)):\n",
        "#         frame = frames[i]\n",
        "#         audio_feature = audio_features[:, i:i+80]  # Assuming 80 frames per second\n",
        "#         audio_feature = torch.FloatTensor(audio_feature).unsqueeze(0).to('cuda')\n",
        "\n",
        "#         # Preprocess the frame to have the expected number of channels (1)\n",
        "#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Convert to grayscale\n",
        "#         frame = torch.FloatTensor(frame).unsqueeze(0).unsqueeze(0).to('cuda') # Add channel dimension\n",
        "\n",
        "#         synced_frame = model(frame, audio_feature)\n",
        "#         synced_frames.append(synced_frame.cpu().numpy())\n",
        "#     return synced_frames\n",
        "\n",
        "# synced_frames = generate_lip_synced_frames(model, frames, audio_features)\n",
        "\n",
        "# Step 3: Load and Run the Model\n",
        "\n",
        "# ... (previous code)\n",
        "\n",
        "# # Function to generate lip-synced frames\n",
        "# def generate_lip_synced_frames(model, frames, audio_features):\n",
        "#     synced_frames = []\n",
        "#     for i in range(len(frames)):\n",
        "#         frame = frames[i]\n",
        "#         audio_feature = audio_features[:, i:i+80]  # Assuming 80 frames per second\n",
        "#         audio_feature = torch.FloatTensor(audio_feature).unsqueeze(0).to('cuda')\n",
        "\n",
        "#         # Preprocess the frame to have the expected number of channels (3 for color images)\n",
        "#         # If the model expects a different number of channels, adjust this accordingly\n",
        "#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Convert to grayscale\n",
        "#         frame = torch.FloatTensor(frame).permute(0,3 ).to('cuda') # Add channel dimension and permute to (batch, channels, height, width)\n",
        "\n",
        "#         synced_frame = model(frame, audio_feature)\n",
        "#         synced_frames.append(synced_frame.cpu().numpy())\n",
        "#     return synced_frames\n",
        "\n",
        "# synced_frames = generate_lip_synced_frames(model, frames, audio_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "YTttsGPb5fN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9yCphLP5gJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSpqqIZp5gFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZZjZdxX5gCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}